 Evaluation Parameters and Data Generation: To generate the test data, we utilized a script provided in the dataGen folder to create .json configuration files for each maze size. These configurations not only specified the maze dimensions but also included details about the number and positions of entry and exit points. This allowed us to explore the impact of the number and placement of entry/exit points on the data structures' performance, For data generation I wrote a python / bash script <appendix X> that generated the json format configuration files. For my testing I ran it locally with no other apps open, and without the visualisation on matplotlib active. I also explored square, vertical, and horizontal mazes... I ran each case 10 times, and checked the variance of the results if the variance was noticeable (+-10%) I ran the tests again to mitigate any bias. mention the uncertainty quantification and monte carlo methods Extending on the requirements for this Assessment I timed, and counted the number of times each function was run, as well as the running time for all the data structures to compare them better please see <appendix X> I decided to do 8 variously sized mazes per type (square, vertical and horizontal). deciding to double the number of cells each time to test a wide array of sizes, this was in effort to ensure we see the real empirical evidence for any trends, and predictions we can make on our algorithms. I then want to use pandas, and plotly express in a python notebook to do some EDA to uncover these trends, and make predictions. In our experimental setup, we aimed to thoroughly evaluate the performance of various data structures in representing and navigating through mazes of different sizes and configurations. square mazes varied from 5x5 -> 200x200, where vertical and horizontal varied from 5x10 -> 5x1600, and 10x5 -> 1600x5 (use proper mathematical notation). e.g. use is an element or domain / codomain whatever you think is best. The maximum value was decided as in my initial testing it exceeded my Gaming Computer's Max RAM Capacity (at least Adjacency Matrix did). Data Structures Evaluated: In our experiments, we implemented three different data structures to represent the mazes:

    Adjacency Matrix
    Adjacency List
    Array (as a baseline). Timing Measurements: To measure the performance of the data structures, we focused on the time taken to perform various maze-related operations, such as finding all neighbors of a given cell, or updating a Wall. I also as previosuly mentioned used import timeit import atexit to time the individual functions as well as the pre-implemented full generation timer built into the skeleton code. All of our final data is in <appendix X.xlsx> which consists of all tested data structures, and maze types. Also appendix the EDA. In summary, our experimental setup aimed to provide a comprehensive evaluation of the selected data structures by considering a range of maze sizes, both square-based and non-square, measurements would allow us to draw insights into the strengths and weaknesses of the different data structures in the context of maze representation and navigation. which will be done in our empirical analysis

